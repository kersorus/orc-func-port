
/* autogenerated from audiomixer_orc_add_volume_s32.orc */

/* audiomixer_orc_add_volume_s32 */
.global audiomixer_orc_add_volume_s32
.p2align 4
audiomixer_orc_add_volume_s32:
# 1: loadpl
# LOOP SHIFT 0
# 0: loadl
# 2: mulslq
# 3: shrsq
# 4: convsssql
# 5: loadl
# 6: addssl
# loading constant 18446744073709551615 0xffffffffffffffff
# loading constant 2147483647 0x        7fffffff
# 7: storel
# LOOP SHIFT 1
# 0: loadl
# 2: mulslq
# 3: shrsq
# 4: convsssql
# 5: loadl
# 6: addssl
# loading constant 18446744073709551615 0xffffffffffffffff
# loading constant 2147483647 0x        7fffffff
# 7: storel
# LOOP SHIFT 2
# 0: loadl
# 2: mulslq
# 3: shrsq
# 4: convsssql
# 5: loadl
# 6: addssl
# loading constant 18446744073709551615 0xffffffffffffffff
# loading constant 2147483647 0x        7fffffff
# 7: storel
# 0: loadl
# 2: mulslq
# 3: shrsq
# 4: convsssql
# 5: loadl
# 6: addssl
# loading constant 18446744073709551615 0xffffffffffffffff
# loading constant 2147483647 0x        7fffffff
# 7: storel
# LOOP SHIFT 2
# 0: loadl
# 2: mulslq
# 3: shrsq
# 4: convsssql
# 5: loadl
# 6: addssl
# loading constant 18446744073709551615 0xffffffffffffffff
# loading constant 2147483647 0x        7fffffff
# 7: storel
# LOOP SHIFT 1
# 0: loadl
# 2: mulslq
# 3: shrsq
# 4: convsssql
# 5: loadl
# 6: addssl
# loading constant 18446744073709551615 0xffffffffffffffff
# loading constant 2147483647 0x        7fffffff
# 7: storel
# LOOP SHIFT 0
# 0: loadl
# 2: mulslq
# 3: shrsq
# 4: convsssql
# 5: loadl
# 6: addssl
# loading constant 18446744073709551615 0xffffffffffffffff
# loading constant 2147483647 0x        7fffffff
# 7: storel
  endbr64 
  vmovd 632(%rdi), %xmm0
  vpbroadcastd %xmm0, %ymm0
  mov $32, %eax
  sub 24(%rdi), %eax
  and $15, %eax
  sar $2, %eax
  cmp %eax, 8(%rdi)
  jle 6f
  movl %eax, 12(%rdi)
  movl 8(%rdi), %ecx
  sub %eax, %ecx
  mov %ecx, %eax
  sar $3, %ecx
  movl %ecx, 16(%rdi)
  and $7, %eax
  movl %eax, 20(%rdi)
  jmp 7f
6:
  movl 8(%rdi), %eax
  movl %eax, 12(%rdi)
  mov $0, %eax
  movl %eax, 16(%rdi)
  movl %eax, 20(%rdi)
7:
  mov 24(%rdi), %rax
  mov 56(%rdi), %rdx
  testl $1, 12(%rdi)
  jz 16f
  vmovd 0(%rdx), %xmm1
  vmovdqa %xmm0, %xmm2
  vpunpckldq %xmm1, %xmm1, %xmm1
  vpunpckldq %xmm2, %xmm2, %xmm2
  vpmuldq %xmm2, %xmm1, %xmm1
  vpshufd $245, %xmm1, %xmm2
  vpsrad $31, %xmm2, %xmm2
  vpsllq $37, %xmm2, %xmm2
  vpsrlq $27, %xmm1, %xmm1
  vpor %xmm2, %xmm1, %xmm1
  vpxor %ymm2, %ymm2, %ymm2
  mov $2147483647, %ecx
  vpinsrd $0, %ecx, %xmm2, %xmm2
  vpbroadcastq %xmm2, %ymm2
  mov $-1, %ecx
  vpinsrd $1, %ecx, %xmm3, %xmm3
  mov $-2147483648, %ecx
  vpinsrd $0, %ecx, %xmm3, %xmm3
  vpbroadcastq %xmm3, %ymm3
  vpcmpgtq %xmm2, %xmm1, %xmm4
  vblendvpd %xmm2, %xmm1, %xmm4, %xmm1
  vpcmpgtq %xmm3, %xmm1, %xmm4
  vblendvpd %xmm1, %xmm3, %xmm4, %xmm1
  vpshufd $216, %xmm1, %xmm1
  vmovd 0(%rax), %xmm2
  vmovdqa %xmm2, %xmm3
  vmovdqa %xmm2, %xmm4
  vpxor %xmm1, %xmm2, %xmm3
  vpaddd %xmm1, %xmm2, %xmm2
  vpxor %xmm2, %xmm4, %xmm4
  mov $-1, %ecx
  vmovd %ecx, %xmm5
  vpbroadcastd %xmm5, %ymm5
  vpxor %xmm5, %xmm4, %xmm4
  vpor %xmm4, %xmm3, %xmm3
  vpsrad $31, %xmm3, %xmm3
  vpsrad $31, %xmm1, %xmm4
  vpand %xmm3, %xmm2, %xmm2
  vpcmpeqb %ymm6, %ymm6, %ymm6
  vpsrld $1, %ymm6, %ymm6
  vpxor %xmm6, %xmm4, %xmm4
  vpandn %xmm4, %xmm3, %xmm3
  vpor %xmm3, %xmm2, %xmm2
  vmovd %xmm2, 0(%rax)
  leaq 4(%rax), %rax
  leaq 4(%rdx), %rdx
16:
  testl $2, 12(%rdi)
  jz 17f
  vmovq 0(%rdx), %xmm1
  vmovdqa %xmm0, %xmm2
  vpunpckldq %xmm1, %xmm1, %xmm1
  vpunpckldq %xmm2, %xmm2, %xmm2
  vpmuldq %xmm2, %xmm1, %xmm1
  vpshufd $245, %xmm1, %xmm2
  vpsrad $31, %xmm2, %xmm2
  vpsllq $37, %xmm2, %xmm2
  vpsrlq $27, %xmm1, %xmm1
  vpor %xmm2, %xmm1, %xmm1
  vpxor %ymm2, %ymm2, %ymm2
  mov $2147483647, %ecx
  vpinsrd $0, %ecx, %xmm2, %xmm2
  vpbroadcastq %xmm2, %ymm2
  mov $-1, %ecx
  vpinsrd $1, %ecx, %xmm3, %xmm3
  mov $-2147483648, %ecx
  vpinsrd $0, %ecx, %xmm3, %xmm3
  vpbroadcastq %xmm3, %ymm3
  vpcmpgtq %xmm2, %xmm1, %xmm4
  vblendvpd %xmm2, %xmm1, %xmm4, %xmm1
  vpcmpgtq %xmm3, %xmm1, %xmm4
  vblendvpd %xmm1, %xmm3, %xmm4, %xmm1
  vpshufd $216, %xmm1, %xmm1
  vmovq 0(%rax), %xmm2
  vmovdqa %xmm2, %xmm3
  vmovdqa %xmm2, %xmm4
  vpxor %xmm1, %xmm2, %xmm3
  vpaddd %xmm1, %xmm2, %xmm2
  vpxor %xmm2, %xmm4, %xmm4
  mov $-1, %ecx
  vmovd %ecx, %xmm5
  vpbroadcastd %xmm5, %ymm5
  vpxor %xmm5, %xmm4, %xmm4
  vpor %xmm4, %xmm3, %xmm3
  vpsrad $31, %xmm3, %xmm3
  vpsrad $31, %xmm1, %xmm4
  vpand %xmm3, %xmm2, %xmm2
  vpcmpeqb %ymm6, %ymm6, %ymm6
  vpsrld $1, %ymm6, %ymm6
  vpxor %xmm6, %xmm4, %xmm4
  vpandn %xmm4, %xmm3, %xmm3
  vpor %xmm3, %xmm2, %xmm2
  vmovq %xmm2, 0(%rax)
  leaq 8(%rax), %rax
  leaq 8(%rdx), %rdx
17:
1:
  cmpb $0, 16(%rdi)
  jz 3f
  movl 16(%rdi), %esi
.p2align 4
2:
  vmovdqu 0(%rdx), %xmm1
  vpunpckhdq %xmm1, %xmm1, %xmm3
  vpunpckldq %xmm1, %xmm1, %xmm1
  vperm2i128 $32, %ymm3, %ymm1, %ymm1
  vpunpckhdq %xmm0, %xmm0, %xmm3
  vpunpckldq %xmm0, %xmm0, %xmm2
  vperm2i128 $32, %ymm3, %ymm2, %ymm2
  vpmuldq %ymm2, %ymm1, %ymm1
  vpshufd $245, %ymm1, %ymm2
  vpsrad $31, %ymm2, %ymm2
  vpsllq $37, %ymm2, %ymm2
  vpsrlq $27, %ymm1, %ymm1
  vpor %ymm2, %ymm1, %ymm1
  vpxor %ymm2, %ymm2, %ymm2
  mov $2147483647, %ecx
  vpinsrd $0, %ecx, %xmm2, %xmm2
  vpbroadcastq %xmm2, %ymm2
  mov $-1, %ecx
  vpinsrd $1, %ecx, %xmm3, %xmm3
  mov $-2147483648, %ecx
  vpinsrd $0, %ecx, %xmm3, %xmm3
  vpbroadcastq %xmm3, %ymm3
  vpcmpgtq %ymm2, %ymm1, %ymm4
  vblendvpd %ymm2, %ymm1, %ymm4, %ymm1
  vpcmpgtq %ymm3, %ymm1, %ymm4
  vblendvpd %ymm1, %ymm3, %ymm4, %ymm1
  vpshufd $216, %ymm1, %ymm1
  vpermq $216, %ymm1, %ymm1
  vmovdqa 0(%rax), %xmm2
  vmovdqa %xmm2, %xmm3
  vmovdqa %xmm2, %xmm4
  vpxor %xmm1, %xmm2, %xmm3
  vpaddd %xmm1, %xmm2, %xmm2
  vpxor %xmm2, %xmm4, %xmm4
  mov $-1, %ecx
  vmovd %ecx, %xmm5
  vpbroadcastd %xmm5, %ymm5
  vpxor %xmm5, %xmm4, %xmm4
  vpor %xmm4, %xmm3, %xmm3
  vpsrad $31, %xmm3, %xmm3
  vpsrad $31, %xmm1, %xmm4
  vpand %xmm3, %xmm2, %xmm2
  vpcmpeqb %ymm6, %ymm6, %ymm6
  vpsrld $1, %ymm6, %ymm6
  vpxor %xmm6, %xmm4, %xmm4
  vpandn %xmm4, %xmm3, %xmm3
  vpor %xmm3, %xmm2, %xmm2
  vmovdqa %xmm2, 0(%rax)
  vmovdqu 16(%rdx), %xmm1
  vpunpckhdq %xmm1, %xmm1, %xmm3
  vpunpckldq %xmm1, %xmm1, %xmm1
  vperm2i128 $32, %ymm3, %ymm1, %ymm1
  vpunpckhdq %xmm0, %xmm0, %xmm3
  vpunpckldq %xmm0, %xmm0, %xmm2
  vperm2i128 $32, %ymm3, %ymm2, %ymm2
  vpmuldq %ymm2, %ymm1, %ymm1
  vpshufd $245, %ymm1, %ymm2
  vpsrad $31, %ymm2, %ymm2
  vpsllq $37, %ymm2, %ymm2
  vpsrlq $27, %ymm1, %ymm1
  vpor %ymm2, %ymm1, %ymm1
  vpxor %ymm2, %ymm2, %ymm2
  mov $2147483647, %ecx
  vpinsrd $0, %ecx, %xmm2, %xmm2
  vpbroadcastq %xmm2, %ymm2
  mov $-1, %ecx
  vpinsrd $1, %ecx, %xmm3, %xmm3
  mov $-2147483648, %ecx
  vpinsrd $0, %ecx, %xmm3, %xmm3
  vpbroadcastq %xmm3, %ymm3
  vpcmpgtq %ymm2, %ymm1, %ymm4
  vblendvpd %ymm2, %ymm1, %ymm4, %ymm1
  vpcmpgtq %ymm3, %ymm1, %ymm4
  vblendvpd %ymm1, %ymm3, %ymm4, %ymm1
  vpshufd $216, %ymm1, %ymm1
  vpermq $216, %ymm1, %ymm1
  vmovdqa 16(%rax), %xmm2
  vmovdqa %xmm2, %xmm3
  vmovdqa %xmm2, %xmm4
  vpxor %xmm1, %xmm2, %xmm3
  vpaddd %xmm1, %xmm2, %xmm2
  vpxor %xmm2, %xmm4, %xmm4
  mov $-1, %ecx
  vmovd %ecx, %xmm5
  vpbroadcastd %xmm5, %ymm5
  vpxor %xmm5, %xmm4, %xmm4
  vpor %xmm4, %xmm3, %xmm3
  vpsrad $31, %xmm3, %xmm3
  vpsrad $31, %xmm1, %xmm4
  vpand %xmm3, %xmm2, %xmm2
  vpcmpeqb %ymm6, %ymm6, %ymm6
  vpsrld $1, %ymm6, %ymm6
  vpxor %xmm6, %xmm4, %xmm4
  vpandn %xmm4, %xmm3, %xmm3
  vpor %xmm3, %xmm2, %xmm2
  vmovdqa %xmm2, 16(%rax)
  leaq 32(%rax), %rax
  leaq 32(%rdx), %rdx
  add $-1, %esi
  jnz 2b
3:
  testl $4, 20(%rdi)
  jz 10f
  vmovdqu 0(%rdx), %xmm1
  vpunpckhdq %xmm1, %xmm1, %xmm3
  vpunpckldq %xmm1, %xmm1, %xmm1
  vperm2i128 $32, %ymm3, %ymm1, %ymm1
  vpunpckhdq %xmm0, %xmm0, %xmm3
  vpunpckldq %xmm0, %xmm0, %xmm2
  vperm2i128 $32, %ymm3, %ymm2, %ymm2
  vpmuldq %ymm2, %ymm1, %ymm1
  vpshufd $245, %ymm1, %ymm2
  vpsrad $31, %ymm2, %ymm2
  vpsllq $37, %ymm2, %ymm2
  vpsrlq $27, %ymm1, %ymm1
  vpor %ymm2, %ymm1, %ymm1
  vpxor %ymm2, %ymm2, %ymm2
  mov $2147483647, %ecx
  vpinsrd $0, %ecx, %xmm2, %xmm2
  vpbroadcastq %xmm2, %ymm2
  mov $-1, %ecx
  vpinsrd $1, %ecx, %xmm3, %xmm3
  mov $-2147483648, %ecx
  vpinsrd $0, %ecx, %xmm3, %xmm3
  vpbroadcastq %xmm3, %ymm3
  vpcmpgtq %ymm2, %ymm1, %ymm4
  vblendvpd %ymm2, %ymm1, %ymm4, %ymm1
  vpcmpgtq %ymm3, %ymm1, %ymm4
  vblendvpd %ymm1, %ymm3, %ymm4, %ymm1
  vpshufd $216, %ymm1, %ymm1
  vpermq $216, %ymm1, %ymm1
  vmovdqu 0(%rax), %xmm2
  vmovdqa %xmm2, %xmm3
  vmovdqa %xmm2, %xmm4
  vpxor %xmm1, %xmm2, %xmm3
  vpaddd %xmm1, %xmm2, %xmm2
  vpxor %xmm2, %xmm4, %xmm4
  mov $-1, %ecx
  vmovd %ecx, %xmm5
  vpbroadcastd %xmm5, %ymm5
  vpxor %xmm5, %xmm4, %xmm4
  vpor %xmm4, %xmm3, %xmm3
  vpsrad $31, %xmm3, %xmm3
  vpsrad $31, %xmm1, %xmm4
  vpand %xmm3, %xmm2, %xmm2
  vpcmpeqb %ymm6, %ymm6, %ymm6
  vpsrld $1, %ymm6, %ymm6
  vpxor %xmm6, %xmm4, %xmm4
  vpandn %xmm4, %xmm3, %xmm3
  vpor %xmm3, %xmm2, %xmm2
  vmovdqu %xmm2, 0(%rax)
  leaq 16(%rax), %rax
  leaq 16(%rdx), %rdx
10:
  testl $2, 20(%rdi)
  jz 9f
  vmovq 0(%rdx), %xmm1
  vmovdqa %xmm0, %xmm2
  vpunpckldq %xmm1, %xmm1, %xmm1
  vpunpckldq %xmm2, %xmm2, %xmm2
  vpmuldq %xmm2, %xmm1, %xmm1
  vpshufd $245, %xmm1, %xmm2
  vpsrad $31, %xmm2, %xmm2
  vpsllq $37, %xmm2, %xmm2
  vpsrlq $27, %xmm1, %xmm1
  vpor %xmm2, %xmm1, %xmm1
  vpxor %ymm2, %ymm2, %ymm2
  mov $2147483647, %ecx
  vpinsrd $0, %ecx, %xmm2, %xmm2
  vpbroadcastq %xmm2, %ymm2
  mov $-1, %ecx
  vpinsrd $1, %ecx, %xmm3, %xmm3
  mov $-2147483648, %ecx
  vpinsrd $0, %ecx, %xmm3, %xmm3
  vpbroadcastq %xmm3, %ymm3
  vpcmpgtq %xmm2, %xmm1, %xmm4
  vblendvpd %xmm2, %xmm1, %xmm4, %xmm1
  vpcmpgtq %xmm3, %xmm1, %xmm4
  vblendvpd %xmm1, %xmm3, %xmm4, %xmm1
  vpshufd $216, %xmm1, %xmm1
  vmovq 0(%rax), %xmm2
  vmovdqa %xmm2, %xmm3
  vmovdqa %xmm2, %xmm4
  vpxor %xmm1, %xmm2, %xmm3
  vpaddd %xmm1, %xmm2, %xmm2
  vpxor %xmm2, %xmm4, %xmm4
  mov $-1, %ecx
  vmovd %ecx, %xmm5
  vpbroadcastd %xmm5, %ymm5
  vpxor %xmm5, %xmm4, %xmm4
  vpor %xmm4, %xmm3, %xmm3
  vpsrad $31, %xmm3, %xmm3
  vpsrad $31, %xmm1, %xmm4
  vpand %xmm3, %xmm2, %xmm2
  vpcmpeqb %ymm6, %ymm6, %ymm6
  vpsrld $1, %ymm6, %ymm6
  vpxor %xmm6, %xmm4, %xmm4
  vpandn %xmm4, %xmm3, %xmm3
  vpor %xmm3, %xmm2, %xmm2
  vmovq %xmm2, 0(%rax)
  leaq 8(%rax), %rax
  leaq 8(%rdx), %rdx
9:
  testl $1, 20(%rdi)
  jz 8f
  vmovd 0(%rdx), %xmm1
  vmovdqa %xmm0, %xmm2
  vpunpckldq %xmm1, %xmm1, %xmm1
  vpunpckldq %xmm2, %xmm2, %xmm2
  vpmuldq %xmm2, %xmm1, %xmm1
  vpshufd $245, %xmm1, %xmm2
  vpsrad $31, %xmm2, %xmm2
  vpsllq $37, %xmm2, %xmm2
  vpsrlq $27, %xmm1, %xmm1
  vpor %xmm2, %xmm1, %xmm1
  vpxor %ymm2, %ymm2, %ymm2
  mov $2147483647, %ecx
  vpinsrd $0, %ecx, %xmm2, %xmm2
  vpbroadcastq %xmm2, %ymm2
  mov $-1, %ecx
  vpinsrd $1, %ecx, %xmm3, %xmm3
  mov $-2147483648, %ecx
  vpinsrd $0, %ecx, %xmm3, %xmm3
  vpbroadcastq %xmm3, %ymm3
  vpcmpgtq %xmm2, %xmm1, %xmm4
  vblendvpd %xmm2, %xmm1, %xmm4, %xmm1
  vpcmpgtq %xmm3, %xmm1, %xmm4
  vblendvpd %xmm1, %xmm3, %xmm4, %xmm1
  vpshufd $216, %xmm1, %xmm1
  vmovd 0(%rax), %xmm2
  vmovdqa %xmm2, %xmm3
  vmovdqa %xmm2, %xmm4
  vpxor %xmm1, %xmm2, %xmm3
  vpaddd %xmm1, %xmm2, %xmm2
  vpxor %xmm2, %xmm4, %xmm4
  mov $-1, %ecx
  vmovd %ecx, %xmm5
  vpbroadcastd %xmm5, %ymm5
  vpxor %xmm5, %xmm4, %xmm4
  vpor %xmm4, %xmm3, %xmm3
  vpsrad $31, %xmm3, %xmm3
  vpsrad $31, %xmm1, %xmm4
  vpand %xmm3, %xmm2, %xmm2
  vpcmpeqb %ymm6, %ymm6, %ymm6
  vpsrld $1, %ymm6, %ymm6
  vpxor %xmm6, %xmm4, %xmm4
  vpandn %xmm4, %xmm3, %xmm3
  vpor %xmm3, %xmm2, %xmm2
  vmovd %xmm2, 0(%rax)
  leaq 4(%rax), %rax
  leaq 4(%rdx), %rdx
8:
  vzeroupper 
  retq 


