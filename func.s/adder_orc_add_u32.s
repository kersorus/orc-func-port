
/* autogenerated from adder_orc_add_u32.orc */

/* adder_orc_add_u32 */
.global adder_orc_add_u32
.p2align 4
adder_orc_add_u32:
# LOOP SHIFT 0
# 0: loadl
# 1: loadl
# 2: addusl
# 3: storel
# LOOP SHIFT 1
# 0: loadl
# 1: loadl
# 2: addusl
# 3: storel
# LOOP SHIFT 2
# 0: loadl
# 1: loadl
# 2: addusl
# 3: storel
# 0: loadl
# 1: loadl
# 2: addusl
# 3: storel
# LOOP SHIFT 2
# 0: loadl
# 1: loadl
# 2: addusl
# 3: storel
# LOOP SHIFT 1
# 0: loadl
# 1: loadl
# 2: addusl
# 3: storel
# LOOP SHIFT 0
# 0: loadl
# 1: loadl
# 2: addusl
# 3: storel
  endbr64 
  mov $16, %eax
  sub 24(%rdi), %eax
  and $15, %eax
  sar $2, %eax
  cmp %eax, 8(%rdi)
  jle 6f
  movl %eax, 12(%rdi)
  movl 8(%rdi), %ecx
  sub %eax, %ecx
  mov %ecx, %eax
  sar $3, %ecx
  movl %ecx, 16(%rdi)
  and $7, %eax
  movl %eax, 20(%rdi)
  jmp 7f
6:
  movl 8(%rdi), %eax
  movl %eax, 12(%rdi)
  mov $0, %eax
  movl %eax, 16(%rdi)
  movl %eax, 20(%rdi)
7:
  mov 24(%rdi), %rax
  mov 56(%rdi), %rdx
  testl $1, 12(%rdi)
  jz 13f
  movd 0(%rax), %xmm0
  movd 0(%rdx), %xmm1
  movdqa %xmm1, %xmm2
  pand %xmm0, %xmm2
  movdqa %xmm1, %xmm3
  pxor %xmm0, %xmm3
  psrld $1, %xmm3
  paddd %xmm3, %xmm2
  psrad $31, %xmm2
  paddd %xmm1, %xmm0
  por %xmm2, %xmm0
  movd %xmm0, 0(%rax)
  leaq 4(%rax), %rax
  leaq 4(%rdx), %rdx
13:
  testl $2, 12(%rdi)
  jz 14f
  movq 0(%rax), %xmm0
  movq 0(%rdx), %xmm1
  movdqa %xmm1, %xmm2
  pand %xmm0, %xmm2
  movdqa %xmm1, %xmm3
  pxor %xmm0, %xmm3
  psrld $1, %xmm3
  paddd %xmm3, %xmm2
  psrad $31, %xmm2
  paddd %xmm1, %xmm0
  por %xmm2, %xmm0
  movq %xmm0, 0(%rax)
  leaq 8(%rax), %rax
  leaq 8(%rdx), %rdx
14:
1:
  cmp $0, 16(%rdi)
  jz 3f
  movl 16(%rdi), %esi
.p2align 4
2:
  movdqa 0(%rax), %xmm0
  movdqu 0(%rdx), %xmm1
  movdqa %xmm1, %xmm2
  pand %xmm0, %xmm2
  movdqa %xmm1, %xmm3
  pxor %xmm0, %xmm3
  psrld $1, %xmm3
  paddd %xmm3, %xmm2
  psrad $31, %xmm2
  paddd %xmm1, %xmm0
  por %xmm2, %xmm0
  movdqa %xmm0, 0(%rax)
  movdqa 16(%rax), %xmm0
  movdqu 16(%rdx), %xmm1
  movdqa %xmm1, %xmm2
  pand %xmm0, %xmm2
  movdqa %xmm1, %xmm3
  pxor %xmm0, %xmm3
  psrld $1, %xmm3
  paddd %xmm3, %xmm2
  psrad $31, %xmm2
  paddd %xmm1, %xmm0
  por %xmm2, %xmm0
  movdqa %xmm0, 16(%rax)
  leaq 32(%rax), %rax
  leaq 32(%rdx), %rdx
  add $-1, %esi
  jnz 2b
3:
  testl $4, 20(%rdi)
  jz 10f
  movdqu 0(%rax), %xmm0
  movdqu 0(%rdx), %xmm1
  movdqa %xmm1, %xmm2
  pand %xmm0, %xmm2
  movdqa %xmm1, %xmm3
  pxor %xmm0, %xmm3
  psrld $1, %xmm3
  paddd %xmm3, %xmm2
  psrad $31, %xmm2
  paddd %xmm1, %xmm0
  por %xmm2, %xmm0
  movdqu %xmm0, 0(%rax)
  leaq 16(%rax), %rax
  leaq 16(%rdx), %rdx
10:
  testl $2, 20(%rdi)
  jz 9f
  movq 0(%rax), %xmm0
  movq 0(%rdx), %xmm1
  movdqa %xmm1, %xmm2
  pand %xmm0, %xmm2
  movdqa %xmm1, %xmm3
  pxor %xmm0, %xmm3
  psrld $1, %xmm3
  paddd %xmm3, %xmm2
  psrad $31, %xmm2
  paddd %xmm1, %xmm0
  por %xmm2, %xmm0
  movq %xmm0, 0(%rax)
  leaq 8(%rax), %rax
  leaq 8(%rdx), %rdx
9:
  testl $1, 20(%rdi)
  jz 8f
  movd 0(%rax), %xmm0
  movd 0(%rdx), %xmm1
  movdqa %xmm1, %xmm2
  pand %xmm0, %xmm2
  movdqa %xmm1, %xmm3
  pxor %xmm0, %xmm3
  psrld $1, %xmm3
  paddd %xmm3, %xmm2
  psrad $31, %xmm2
  paddd %xmm1, %xmm0
  por %xmm2, %xmm0
  movd %xmm0, 0(%rax)
  leaq 4(%rax), %rax
  leaq 4(%rdx), %rdx
8:
  retq 


